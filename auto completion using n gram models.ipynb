{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9186,"sourceType":"datasetVersion","datasetId":6261}],"dockerImageVersionId":30042,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%capture\n\n## Importing Packages\nimport math\nimport nltk\nimport random\nimport pickle\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\n\n## Basic File Paths\ndata_dir = \"../input/tweets-blogs-news-swiftkey-dataset-4million/final/en_US\"\nfile_path = data_dir + \"/en_US.twitter.txt\"\n\n## nltk settings\nnltk.data.path.append(data_dir)\nnltk.download('punkt')\n\n## Opening the File in read mode (\"r\")\nwith open(file_path, \"r\") as f:\n    data = f.read()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-09-01T23:28:13.730684Z","iopub.execute_input":"2024-09-01T23:28:13.731199Z","iopub.status.idle":"2024-09-01T23:28:40.105185Z","shell.execute_reply.started":"2024-09-01T23:28:13.731157Z","shell.execute_reply":"2024-09-01T23:28:40.104022Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"def preprocess_pipeline(data) -> 'list':\n\n    # Split by newline character\n    sentences = data.split('\\n')\n    \n    # Remove leading and trailing spaces\n    sentences = [s.strip() for s in sentences]\n    \n    # Drop Empty Sentences\n    sentences = [s for s in sentences if len(s) > 0]\n    \n    # Empty List to hold Tokenized Sentences\n    tokenized = []\n    \n    # Iterate through sentences\n    for sentence in sentences:\n        \n        # Convert to lowercase\n        sentence = sentence.lower()\n        \n        # Convert to a list of words\n        token = nltk.word_tokenize(sentence)\n        \n        # Append to list\n        tokenized.append(token)\n        \n    return tokenized\n\n\n## Pass our data to this function    \ntokenized_sentences = preprocess_pipeline(data)","metadata":{"execution":{"iopub.status.busy":"2024-09-01T23:28:40.107934Z","iopub.execute_input":"2024-09-01T23:28:40.108430Z","iopub.status.idle":"2024-09-01T23:41:13.142613Z","shell.execute_reply.started":"2024-09-01T23:28:40.108359Z","shell.execute_reply":"2024-09-01T23:41:13.141523Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"## Obtain Train and Test Split \ntrain, test = train_test_split(tokenized_sentences, test_size=0.2, random_state=42)\n\n## Obtain Train and Validation Split \ntrain, val = train_test_split(train, test_size=0.25, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2024-09-01T23:41:13.144288Z","iopub.execute_input":"2024-09-01T23:41:13.144636Z","iopub.status.idle":"2024-09-01T23:41:16.054023Z","shell.execute_reply.started":"2024-09-01T23:41:13.144602Z","shell.execute_reply":"2024-09-01T23:41:16.053054Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"def count_the_words(sentences) -> 'dict':\n    \n  # Creating a Dictionary of counts\n  word_counts = {}\n\n  # Iterating over sentences\n  for sentence in sentences:\n    \n    # Iterating over Tokens\n    for token in sentence:\n    \n      # Add count for new word\n      if token not in word_counts.keys():\n        word_counts[token] = 1\n        \n      # Increase count by one\n      else:\n        word_counts[token] += 1\n        \n  return word_counts","metadata":{"execution":{"iopub.status.busy":"2024-09-01T23:41:16.055380Z","iopub.execute_input":"2024-09-01T23:41:16.055701Z","iopub.status.idle":"2024-09-01T23:41:16.265992Z","shell.execute_reply.started":"2024-09-01T23:41:16.055669Z","shell.execute_reply":"2024-09-01T23:41:16.264532Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"def handling_oov(tokenized_sentences, count_threshold) -> 'list':\n\n  # Empty list for closed vocabulary\n  closed_vocabulary = []\n\n  # Obtain frequency dictionary using previously defined function\n  words_count = count_the_words(tokenized_sentences)\n    \n  # Iterate over words and counts \n  for word, count in words_count.items():\n    \n    # Append if it's more(or equal) to the threshold \n    if count >= count_threshold :\n      closed_vocabulary.append(word)\n\n  return closed_vocabulary","metadata":{"execution":{"iopub.status.busy":"2024-09-01T23:41:16.270088Z","iopub.execute_input":"2024-09-01T23:41:16.270468Z","iopub.status.idle":"2024-09-01T23:41:16.280396Z","shell.execute_reply.started":"2024-09-01T23:41:16.270430Z","shell.execute_reply":"2024-09-01T23:41:16.279089Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"def unk_tokenize(tokenized_sentences, vocabulary, unknown_token = \"<unk>\") -> 'list':\n\n  # Convert Vocabulary into a set\n  vocabulary = set(vocabulary)\n\n  # Create empty list for sentences\n  new_tokenized_sentences = []\n  \n  # Iterate over sentences\n  for sentence in tokenized_sentences:\n\n    # Iterate over sentence and add <unk> \n    # if the token is absent from the vocabulary\n    new_sentence = []\n    for token in sentence:\n      if token in vocabulary:\n        new_sentence.append(token)\n      else:\n        new_sentence.append(unknown_token)\n    \n    # Append sentece to the new list\n    new_tokenized_sentences.append(new_sentence)\n\n  return new_tokenized_sentences","metadata":{"execution":{"iopub.status.busy":"2024-09-01T23:41:16.282614Z","iopub.execute_input":"2024-09-01T23:41:16.283085Z","iopub.status.idle":"2024-09-01T23:41:16.297720Z","shell.execute_reply.started":"2024-09-01T23:41:16.283031Z","shell.execute_reply":"2024-09-01T23:41:16.296668Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"def cleansing(train_data, test_data, count_threshold):\n    \n  # Get closed Vocabulary\n  vocabulary = handling_oov(train_data, count_threshold)\n    \n  # Updated Training Dataset\n  new_train_data = unk_tokenize(train_data, vocabulary)\n    \n  # Updated Test Dataset\n  new_test_data = unk_tokenize(test_data, vocabulary)\n\n  return new_train_data, new_test_data, vocabulary","metadata":{"execution":{"iopub.status.busy":"2024-09-01T23:41:16.299684Z","iopub.execute_input":"2024-09-01T23:41:16.300192Z","iopub.status.idle":"2024-09-01T23:41:16.311053Z","shell.execute_reply.started":"2024-09-01T23:41:16.300139Z","shell.execute_reply":"2024-09-01T23:41:16.310016Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"min_freq = 6\nfinal_train, final_test, vocabulary = cleansing(train, test, min_freq)","metadata":{"execution":{"iopub.status.busy":"2024-09-01T23:41:16.312648Z","iopub.execute_input":"2024-09-01T23:41:16.313145Z","iopub.status.idle":"2024-09-01T23:41:44.699436Z","shell.execute_reply.started":"2024-09-01T23:41:16.313098Z","shell.execute_reply":"2024-09-01T23:41:44.697967Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"def count_n_grams(data, n, start_token = \"<s>\", end_token = \"<e>\") -> 'dict':\n\n  # Empty dict for n-grams\n  n_grams = {}\n \n  # Iterate over all sentences in the dataset\n  for sentence in data:\n        \n    # Append n start tokens and a single end token to the sentence\n    sentence = [start_token]*n + sentence + [end_token]\n    \n    # Convert the sentence into a tuple\n    sentence = tuple(sentence)\n\n    # Temp var to store length from start of n-gram to end\n    m = len(sentence) if n==1 else len(sentence)-1\n    \n    # Iterate over this length\n    for i in range(m):\n        \n      # Get the n-gram\n      n_gram = sentence[i:i+n]\n    \n      # Add the count of n-gram as value to our dictionary\n      # IF n-gram is already present\n      if n_gram in n_grams.keys():\n        n_grams[n_gram] += 1\n      # Add n-gram count\n      else:\n        n_grams[n_gram] = 1\n        \n  return n_grams","metadata":{"execution":{"iopub.status.busy":"2024-09-01T23:41:44.701251Z","iopub.execute_input":"2024-09-01T23:41:44.701638Z","iopub.status.idle":"2024-09-01T23:41:44.712958Z","shell.execute_reply.started":"2024-09-01T23:41:44.701598Z","shell.execute_reply":"2024-09-01T23:41:44.711671Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"def prob_for_single_word(word, previous_n_gram, n_gram_counts, nplus1_gram_counts, vocabulary_size, k = 1.0) -> 'float':\n\n  # Convert the previous_n_gram into a tuple \n  previous_n_gram = tuple(previous_n_gram)\n    \n  # Calculating the count, if exists from our freq dictionary otherwise zero\n  previous_n_gram_count = n_gram_counts[previous_n_gram] if previous_n_gram in n_gram_counts else 0\n  \n  # The Denominator\n  denom = previous_n_gram_count + k * vocabulary_size\n\n  # previous n-gram plus the current word as a tuple\n  nplus1_gram = previous_n_gram + (word,)\n\n  # Calculating the nplus1 count, if exists from our freq dictionary otherwise zero \n  nplus1_gram_count = nplus1_gram_counts[nplus1_gram] if nplus1_gram in nplus1_gram_counts else 0\n\n  # Numerator\n  num = nplus1_gram_count + k\n\n  # Final Fraction\n  prob = num / denom\n  return prob","metadata":{"execution":{"iopub.status.busy":"2024-09-01T23:41:44.714275Z","iopub.execute_input":"2024-09-01T23:41:44.714613Z","iopub.status.idle":"2024-09-01T23:41:44.733065Z","shell.execute_reply.started":"2024-09-01T23:41:44.714580Z","shell.execute_reply":"2024-09-01T23:41:44.731644Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"def probs(previous_n_gram, n_gram_counts, nplus1_gram_counts, vocabulary, k=1.0) -> 'dict':\n\n  # Convert to Tuple\n  previous_n_gram = tuple(previous_n_gram)\n\n  # Add end and unknown tokens to the vocabulary\n  vocabulary = vocabulary + [\"<e>\", \"<unk>\"]\n\n  # Calculate the size of the vocabulary\n  vocabulary_size = len(vocabulary)\n\n  # Empty dict for probabilites\n  probabilities = {}\n\n  # Iterate over words \n  for word in vocabulary:\n    \n    # Calculate probability\n    probability = prob_for_single_word(word, previous_n_gram, \n                                           n_gram_counts, nplus1_gram_counts, \n                                           vocabulary_size, k=k)\n    # Create mapping: word -> probability\n    probabilities[word] = probability\n\n  return probabilities","metadata":{"execution":{"iopub.status.busy":"2024-09-01T23:41:44.734706Z","iopub.execute_input":"2024-09-01T23:41:44.735225Z","iopub.status.idle":"2024-09-01T23:41:44.748133Z","shell.execute_reply.started":"2024-09-01T23:41:44.735171Z","shell.execute_reply":"2024-09-01T23:41:44.746979Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"def auto_complete(previous_tokens, n_gram_counts, nplus1_gram_counts, vocabulary, k=1.0, start_with=None):\n\n    \n    # length of previous words\n    n = len(list(n_gram_counts.keys())[0]) \n    \n    # most recent 'n' words\n    previous_n_gram = previous_tokens[-n:]\n    \n    # Calculate probabilty for all words\n    probabilities = probs(previous_n_gram,n_gram_counts, nplus1_gram_counts,vocabulary, k=k)\n\n    # Intialize the suggestion and max probability\n    suggestion = None\n    max_prob = 0\n\n    # Iterate over all words and probabilites, returning the max.\n    # We also add a check if the start_with parameter is provided\n    for word, prob in probabilities.items():\n        \n        if start_with != None: \n            \n            if not word.startswith(start_with):\n                continue \n\n        if prob > max_prob: \n\n            suggestion = word\n            max_prob = prob\n\n    return suggestion, max_prob","metadata":{"execution":{"iopub.status.busy":"2024-09-01T23:41:44.749757Z","iopub.execute_input":"2024-09-01T23:41:44.750211Z","iopub.status.idle":"2024-09-01T23:41:44.766275Z","shell.execute_reply.started":"2024-09-01T23:41:44.750165Z","shell.execute_reply":"2024-09-01T23:41:44.764892Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"def get_suggestions(previous_tokens, n_gram_counts_list, vocabulary, k=1.0, start_with=None):\n\n    # See how many models we have\n    count = len(n_gram_counts_list)\n    \n    # Empty list for suggestions\n    suggestions = []\n    \n    # IMP: Earlier \"-1\"\n    \n    # Loop over counts\n    for i in range(count-1):\n        \n        # get n and nplus1 counts\n        n_gram_counts = n_gram_counts_list[i]\n        nplus1_gram_counts = n_gram_counts_list[i+1]\n        \n        # get suggestions \n        suggestion = auto_complete(previous_tokens, n_gram_counts,\n                                    nplus1_gram_counts, vocabulary,\n                                    k=k, start_with=start_with)\n        # Append to list\n        suggestions.append(suggestion)\n        \n    return suggestions","metadata":{"execution":{"iopub.status.busy":"2024-09-01T23:41:44.767714Z","iopub.execute_input":"2024-09-01T23:41:44.768139Z","iopub.status.idle":"2024-09-01T23:41:44.779292Z","shell.execute_reply.started":"2024-09-01T23:41:44.768102Z","shell.execute_reply":"2024-09-01T23:41:44.777880Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"n_gram_counts_list = []\nfor n in range(1, 6):\n    n_model_counts = count_n_grams(final_train, n)\n    n_gram_counts_list.append(n_model_counts)","metadata":{"execution":{"iopub.status.busy":"2024-09-01T23:41:44.780990Z","iopub.execute_input":"2024-09-01T23:41:44.781334Z","iopub.status.idle":"2024-09-01T23:44:57.509183Z","shell.execute_reply.started":"2024-09-01T23:41:44.781302Z","shell.execute_reply":"2024-09-01T23:44:57.508036Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"previous_tokens = [\"i\", \"was\", \"about\"]\nsuggestion = get_suggestions(previous_tokens, n_gram_counts_list, vocabulary, k=1.0)\n\ndisplay(suggestion)","metadata":{"execution":{"iopub.status.busy":"2024-09-01T23:44:57.510728Z","iopub.execute_input":"2024-09-01T23:44:57.511103Z","iopub.status.idle":"2024-09-01T23:44:59.013746Z","shell.execute_reply.started":"2024-09-01T23:44:57.511065Z","shell.execute_reply":"2024-09-01T23:44:59.012519Z"},"trusted":true},"execution_count":15,"outputs":[{"output_type":"display_data","data":{"text/plain":"[('the', 0.0529197757731002),\n ('to', 0.0027387051700046576),\n ('to', 0.0018524062570166903),\n ('lol', 1.8754688672168043e-05)]"},"metadata":{}}]},{"cell_type":"code","source":"print(\"unigram count:\" , len(n_gram_counts_list[0]))\nprint(\"bigram count:\", len(n_gram_counts_list[1]))\nprint(\"trigram count:\", len(n_gram_counts_list[2]))\nprint(\"quadgram count:\", len(n_gram_counts_list[3]))\nprint(\"quintgram count:\", len(n_gram_counts_list[4]))","metadata":{"execution":{"iopub.status.busy":"2024-09-01T23:44:59.015463Z","iopub.execute_input":"2024-09-01T23:44:59.015921Z","iopub.status.idle":"2024-09-01T23:44:59.024347Z","shell.execute_reply.started":"2024-09-01T23:44:59.015881Z","shell.execute_reply":"2024-09-01T23:44:59.023267Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"unigram count: 53321\nbigram count: 2884498\ntrigram count: 9810186\nquadgram count: 15666285\nquintgram count: 19070234\n","output_type":"stream"}]},{"cell_type":"code","source":"# Storing to file\nwith open(\"en_counts.txt\", 'wb') as f:\n    pickle.dump(n_gram_counts_list, f)","metadata":{"execution":{"iopub.status.busy":"2024-09-01T23:44:59.026226Z","iopub.execute_input":"2024-09-01T23:44:59.026589Z","iopub.status.idle":"2024-09-01T23:45:58.082273Z","shell.execute_reply.started":"2024-09-01T23:44:59.026553Z","shell.execute_reply":"2024-09-01T23:45:58.080995Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"# Storing to file\nwith open(\"vocab.txt\", 'wb') as f:\n    pickle.dump(vocabulary, f)","metadata":{"execution":{"iopub.status.busy":"2024-09-01T23:45:58.083728Z","iopub.execute_input":"2024-09-01T23:45:58.084085Z","iopub.status.idle":"2024-09-01T23:45:58.110923Z","shell.execute_reply.started":"2024-09-01T23:45:58.084051Z","shell.execute_reply":"2024-09-01T23:45:58.109861Z"},"trusted":true},"execution_count":18,"outputs":[]}]}